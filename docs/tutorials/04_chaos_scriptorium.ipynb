{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Surviving the Chaos Scriptorium Benchmark\n",
    "\n",
    "**Goal**: Test agent resilience in a volatile environment\n",
    "\n",
    "**Time**: 25 minutes\n",
    "\n",
    "**Prerequisites**: Tutorials 1-3\n",
    "\n",
    "---\n",
    "\n",
    "## What is Chaos Scriptorium?\n",
    "\n",
    "The Chaos Scriptorium is a benchmark that simulates **environmental volatility**:\n",
    "\n",
    "- **Goal**: Find a secret key at `/root/data/vault/key.txt`\n",
    "- **Challenge**: File permissions randomly change every 3 steps\n",
    "- **Tools**: Different tools have different success rates when files are locked\n",
    "\n",
    "This models real-world scenarios:\n",
    "- APIs changing behavior\n",
    "- Services going down\n",
    "- Permissions shifting\n",
    "- Rate limits kicking in\n",
    "\n",
    "**Key insight**: Standard agents loop forever. LRS agents adapt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LRS-Agents if not already installed\n",
    "# !pip install lrs-agents\n",
    "\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lrs.benchmarks.chaos_scriptorium import (\n",
    "    ChaosEnvironment,\n",
    "    ShellTool,\n",
    "    PythonTool,\n",
    "    FileReadTool,\n",
    "    run_benchmark\n",
    ")\n",
    "from lrs import create_lrs_agent\n",
    "from lrs.monitoring.tracker import LRSStateTracker\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for the chaos environment\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Created test environment at: {temp_dir}\")\n",
    "\n",
    "# Initialize chaos environment\n",
    "env = ChaosEnvironment(\n",
    "    root_dir=temp_dir,\n",
    "    chaos_interval=3,  # Change permissions every 3 steps\n",
    "    lock_probability=0.5  # 50% chance of locking on each chaos tick\n",
    ")\n",
    "\n",
    "# Create the directory structure\n",
    "env.setup()\n",
    "\n",
    "print(\"\\nüìÅ Directory structure created:\")\n",
    "print(f\"  {temp_dir}/\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ data/\")\n",
    "print(f\"      ‚îî‚îÄ‚îÄ vault/\")\n",
    "print(f\"          ‚îî‚îÄ‚îÄ key.txt  ‚Üê SECRET KEY HERE\")\n",
    "\n",
    "print(\"\\nüé≤ Chaos settings:\")\n",
    "print(f\"  - Permissions change every {env.chaos_interval} steps\")\n",
    "print(f\"  - Lock probability: {env.lock_probability * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Tools\n",
    "\n",
    "Three tools with different reliability under lock conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrs.core.lens import ToolLens, ExecutionResult\n",
    "import subprocess\n",
    "\n",
    "class ShellTool(ToolLens):\n",
    "    \"\"\"\n",
    "    Execute shell commands.\n",
    "    \n",
    "    Performance:\n",
    "    - Unlocked: 95% success\n",
    "    - Locked: 40% success (struggles with permissions)\n",
    "    \"\"\"\n",
    "    def __init__(self, env: ChaosEnvironment):\n",
    "        super().__init__(\n",
    "            name=\"shell_exec\",\n",
    "            input_schema={'type': 'object', 'required': ['command']},\n",
    "            output_schema={'type': 'string'}\n",
    "        )\n",
    "        self.env = env\n",
    "    \n",
    "    def get(self, state: dict) -> ExecutionResult:\n",
    "        self.call_count += 1\n",
    "        command = state.get('command', 'ls')\n",
    "        \n",
    "        # Check if files are locked\n",
    "        if self.env.is_locked() and random.random() < 0.6:\n",
    "            # 60% failure rate when locked\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(\n",
    "                success=False,\n",
    "                value=None,\n",
    "                error=\"Permission denied\",\n",
    "                prediction_error=0.9  # High surprise\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                command,\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=5\n",
    "            )\n",
    "            return ExecutionResult(\n",
    "                success=result.returncode == 0,\n",
    "                value=result.stdout,\n",
    "                error=result.stderr if result.returncode != 0 else None,\n",
    "                prediction_error=0.05 if result.returncode == 0 else 0.8\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(False, None, str(e), 0.95)\n",
    "    \n",
    "    def set(self, state: dict, observation: str) -> dict:\n",
    "        return {**state, 'shell_output': observation}\n",
    "\n",
    "\n",
    "class PythonTool(ToolLens):\n",
    "    \"\"\"\n",
    "    Execute Python code.\n",
    "    \n",
    "    Performance:\n",
    "    - Unlocked: 90% success\n",
    "    - Locked: 80% success (better than shell, can work around permissions)\n",
    "    \"\"\"\n",
    "    def __init__(self, env: ChaosEnvironment):\n",
    "        super().__init__(\n",
    "            name=\"python_exec\",\n",
    "            input_schema={'type': 'object', 'required': ['code']},\n",
    "            output_schema={'type': 'string'}\n",
    "        )\n",
    "        self.env = env\n",
    "    \n",
    "    def get(self, state: dict) -> ExecutionResult:\n",
    "        self.call_count += 1\n",
    "        code = state.get('code', 'print(\"Hello\")')\n",
    "        \n",
    "        # Python is more resilient to locks\n",
    "        if self.env.is_locked() and random.random() < 0.2:\n",
    "            # Only 20% failure rate when locked\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(False, None, \"Access error\", 0.7)\n",
    "        \n",
    "        try:\n",
    "            # Execute in restricted namespace\n",
    "            local_vars = {}\n",
    "            exec(code, {\"__builtins__\": __builtins__}, local_vars)\n",
    "            result = local_vars.get('result', 'Executed')\n",
    "            return ExecutionResult(True, str(result), None, 0.1)\n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(False, None, str(e), 0.8)\n",
    "    \n",
    "    def set(self, state: dict, observation: str) -> dict:\n",
    "        return {**state, 'python_output': observation}\n",
    "\n",
    "\n",
    "class FileReadTool(ToolLens):\n",
    "    \"\"\"\n",
    "    Direct file reading.\n",
    "    \n",
    "    Performance:\n",
    "    - Unlocked: 100% success\n",
    "    - Locked: 0% success (completely fails when locked)\n",
    "    \"\"\"\n",
    "    def __init__(self, env: ChaosEnvironment):\n",
    "        super().__init__(\n",
    "            name=\"file_read\",\n",
    "            input_schema={'type': 'object', 'required': ['path']},\n",
    "            output_schema={'type': 'string'}\n",
    "        )\n",
    "        self.env = env\n",
    "    \n",
    "    def get(self, state: dict) -> ExecutionResult:\n",
    "        self.call_count += 1\n",
    "        path = state.get('path', '')\n",
    "        \n",
    "        # Completely fails when locked\n",
    "        if self.env.is_locked():\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(False, None, \"File locked\", 1.0)\n",
    "        \n",
    "        try:\n",
    "            content = Path(path).read_text()\n",
    "            return ExecutionResult(True, content, None, 0.0)\n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            return ExecutionResult(False, None, str(e), 0.95)\n",
    "    \n",
    "    def set(self, state: dict, observation: str) -> dict:\n",
    "        return {**state, 'file_content': observation}\n",
    "\n",
    "\n",
    "# Create tools\n",
    "tools = [\n",
    "    ShellTool(env),\n",
    "    PythonTool(env),\n",
    "    FileReadTool(env)\n",
    "]\n",
    "\n",
    "print(\"\\nüîß Tools created:\")\n",
    "print(\"  1. ShellTool - Fast but brittle under lock\")\n",
    "print(\"  2. PythonTool - Slower but resilient\")\n",
    "print(\"  3. FileReadTool - Perfect when unlocked, useless when locked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Baseline - Standard Agent (Manual Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_agent(env, max_steps=15):\n",
    "    \"\"\"\n",
    "    Simulate a standard agent that doesn't adapt.\n",
    "    \n",
    "    Strategy: Always use ShellTool, retry on failure.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Standard Agent (No Adaptation)\\n\")\n",
    "    \n",
    "    shell = ShellTool(env)\n",
    "    \n",
    "    for step in range(1, max_steps + 1):\n",
    "        # Chaos tick\n",
    "        env.tick()\n",
    "        \n",
    "        # Always try shell\n",
    "        result = shell.get({'command': f'cat {env.key_path}'})\n",
    "        \n",
    "        status = \"‚úì\" if result.success else \"‚úó\"\n",
    "        print(f\"[Step {step}] {status} ShellExec \", end=\"\")\n",
    "        \n",
    "        if env.is_locked():\n",
    "            print(\"(LOCKED)\", end=\"\")\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\" ‚Üí SUCCESS! Key: {result.value[:20]}...\")\n",
    "            return step\n",
    "        else:\n",
    "            print(f\" ‚Üí {result.error}\")\n",
    "            # Standard agent just retries same action\n",
    "    \n",
    "    print(\"\\n‚ùå FAILED - Timeout\")\n",
    "    return None\n",
    "\n",
    "# Run baseline\n",
    "env.reset()\n",
    "standard_steps = run_standard_agent(env)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STANDARD AGENT RESULT\")\n",
    "print(\"=\"*60)\n",
    "if standard_steps:\n",
    "    print(f\"‚úì Succeeded in {standard_steps} steps (got lucky)\")\n",
    "else:\n",
    "    print(\"‚úó Failed - Looped on same action until timeout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LRS Agent - With Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrs import create_lrs_agent\n",
    "from lrs.core.registry import ToolRegistry\n",
    "from unittest.mock import Mock\n",
    "\n",
    "# Reset environment\n",
    "env.reset()\n",
    "\n",
    "# Create fresh tools\n",
    "shell = ShellTool(env)\n",
    "python = PythonTool(env)\n",
    "file_read = FileReadTool(env)\n",
    "\n",
    "# Create registry with alternatives\n",
    "registry = ToolRegistry()\n",
    "registry.register(shell, alternatives=[\"python_exec\"])\n",
    "registry.register(python, alternatives=[\"file_read\"])\n",
    "registry.register(file_read)\n",
    "\n",
    "# Create LRS agent\n",
    "mock_llm = Mock()\n",
    "\n",
    "from lrs.integration.langgraph import LRSGraphBuilder\n",
    "\n",
    "builder = LRSGraphBuilder(\n",
    "    llm=mock_llm,\n",
    "    registry=registry,\n",
    "    preferences={\n",
    "        'key_found': 10.0,\n",
    "        'error': -3.0,\n",
    "        'step_cost': -0.1\n",
    "    },\n",
    "    use_llm_proposals=False  # Use exhaustive search for this demo\n",
    ")\n",
    "\n",
    "agent = builder.build()\n",
    "\n",
    "# Track state\n",
    "tracker = LRSStateTracker()\n",
    "\n",
    "print(\"‚úÖ LRS Agent created with:\")\n",
    "print(\"  - 3 tools (shell, python, file_read)\")\n",
    "print(\"  - Alternative chains registered\")\n",
    "print(\"  - Precision tracking enabled\")\n",
    "print(\"\\nüöÄ Starting execution...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lrs_agent(agent, env, tracker, max_steps=15):\n",
    "    \"\"\"\n",
    "    Run LRS agent with adaptation.\n",
    "    \"\"\"\n",
    "    print(\"üß† LRS Agent (Active Inference)\\n\")\n",
    "    \n",
    "    # Initial state\n",
    "    state = {\n",
    "        'messages': [{'role': 'user', 'content': f'Find key at {env.key_path}'}],\n",
    "        'belief_state': {'goal': 'find_key', 'key_path': env.key_path},\n",
    "        'precision': {},\n",
    "        'prediction_errors': {},\n",
    "        'tool_history': [],\n",
    "        'adaptation_count': 0\n",
    "    }\n",
    "    \n",
    "    for step in range(1, max_steps + 1):\n",
    "        # Chaos tick\n",
    "        env.tick()\n",
    "        \n",
    "        # Agent decides and executes\n",
    "        state = agent.invoke(state)\n",
    "        \n",
    "        # Track state\n",
    "        tracker.track_state(state)\n",
    "        \n",
    "        # Get latest execution\n",
    "        if state['tool_history']:\n",
    "            latest = state['tool_history'][-1]\n",
    "            \n",
    "            status = \"‚úì\" if latest['success'] else \"‚úó\"\n",
    "            tool = latest['tool']\n",
    "            error = latest.get('prediction_error', 0)\n",
    "            prec = state['precision'].get('execution', 0.5)\n",
    "            \n",
    "            print(f\"[Step {step}] {status} {tool} \", end=\"\")\n",
    "            \n",
    "            if env.is_locked():\n",
    "                print(\"(LOCKED) \", end=\"\")\n",
    "            \n",
    "            print(f\"| Œµ={error:.2f}, Œ≥={prec:.2f}\")\n",
    "            \n",
    "            # Check for adaptation\n",
    "            if state['adaptation_count'] > 0 and step > 1:\n",
    "                prev_count = tracker.history[-2].get('adaptation_count', 0) if len(tracker.history) > 1 else 0\n",
    "                if state['adaptation_count'] > prev_count:\n",
    "                    print(\"    üîÑ ADAPTATION: Precision collapsed, replanning...\")\n",
    "            \n",
    "            # Check success\n",
    "            if latest['success'] and 'key' in str(latest.get('result', '')).lower():\n",
    "                print(f\"\\n‚úÖ SUCCESS! Key found in {step} steps\")\n",
    "                return step\n",
    "    \n",
    "    print(\"\\n‚ùå FAILED - Timeout\")\n",
    "    return None\n",
    "\n",
    "# Run LRS agent\n",
    "lrs_steps = run_lrs_agent(agent, env, tracker)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LRS AGENT RESULT\")\n",
    "print(\"=\"*60)\n",
    "if lrs_steps:\n",
    "    print(f\"‚úì Succeeded in {lrs_steps} steps\")\n",
    "    print(f\"  Adaptations: {tracker.history[-1]['adaptation_count']}\")\n",
    "    print(f\"  Tools used: {len(set(h['tool'] for h in tracker.history[-1]['tool_history']))}\")\n",
    "else:\n",
    "    print(\"‚úó Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Precision Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract precision history\n",
    "precision_history = [\n",
    "    state.get('precision', {}).get('execution', 0.5)\n",
    "    for state in tracker.history\n",
    "]\n",
    "\n",
    "# Extract prediction errors\n",
    "error_history = []\n",
    "for state in tracker.history:\n",
    "    if state.get('tool_history'):\n",
    "        error_history.append(state['tool_history'][-1].get('prediction_error', 0))\n",
    "    else:\n",
    "        error_history.append(0)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Precision trajectory\n",
    "ax1.plot(precision_history, marker='o', linewidth=2, color='blue', label='Execution Precision (Œ≥)')\n",
    "ax1.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='High confidence')\n",
    "ax1.axhline(y=0.4, color='orange', linestyle='--', alpha=0.5, label='Adaptation threshold')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Precision (Œ≥)')\n",
    "ax1.set_title('LRS Agent: Precision Trajectory in Chaos Scriptorium')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Prediction errors\n",
    "ax2.bar(range(len(error_history)), error_history, color='red', alpha=0.6)\n",
    "ax2.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='High surprise')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Prediction Error (Œµ)')\n",
    "ax2.set_title('Prediction Errors (Surprise Events)')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"  - Precision drops when tools fail (high Œµ)\")\n",
    "print(\"  - When Œ≥ < 0.4, agent triggers adaptation\")\n",
    "print(\"  - Precision recovers when new tools succeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Full Benchmark (100 Trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_benchmark(num_trials=100):\n",
    "    \"\"\"\n",
    "    Run benchmark comparing standard vs LRS agents.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß™ Running {num_trials} trials...\\n\")\n",
    "    \n",
    "    standard_results = []\n",
    "    lrs_results = []\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        if (trial + 1) % 10 == 0:\n",
    "            print(f\"  Trial {trial + 1}/{num_trials}...\")\n",
    "        \n",
    "        # Standard agent\n",
    "        env.reset()\n",
    "        standard_success = run_standard_agent(env, max_steps=20) is not None\n",
    "        standard_results.append(standard_success)\n",
    "        \n",
    "        # LRS agent\n",
    "        env.reset()\n",
    "        # Create fresh agent for each trial\n",
    "        agent = builder.build()\n",
    "        tracker = LRSStateTracker()\n",
    "        lrs_success = run_lrs_agent(agent, env, tracker, max_steps=20) is not None\n",
    "        lrs_results.append(lrs_success)\n",
    "    \n",
    "    # Aggregate results\n",
    "    standard_rate = sum(standard_results) / len(standard_results)\n",
    "    lrs_rate = sum(lrs_results) / len(lrs_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nStandard Agent:\")\n",
    "    print(f\"  Success rate: {standard_rate:.1%}\")\n",
    "    print(f\"  Successes: {sum(standard_results)}/{num_trials}\")\n",
    "    \n",
    "    print(f\"\\nLRS Agent:\")\n",
    "    print(f\"  Success rate: {lrs_rate:.1%}\")\n",
    "    print(f\"  Successes: {sum(lrs_results)}/{num_trials}\")\n",
    "    \n",
    "    improvement = ((lrs_rate - standard_rate) / standard_rate) * 100 if standard_rate > 0 else float('inf')\n",
    "    print(f\"\\nüìà Improvement: +{improvement:.0f}%\")\n",
    "    \n",
    "    return standard_rate, lrs_rate\n",
    "\n",
    "# Run benchmark (warning: takes ~5-10 minutes)\n",
    "# Uncomment to run:\n",
    "# standard_rate, lrs_rate = run_full_benchmark(num_trials=100)\n",
    "\n",
    "# For quick demo, use fewer trials:\n",
    "standard_rate, lrs_rate = run_full_benchmark(num_trials=10)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "agents = ['Standard\\nAgent', 'LRS\\nAgent']\n",
    "rates = [standard_rate * 100, lrs_rate * 100]\n",
    "\n",
    "bars = ax.bar(agents, rates, color=['red', 'green'], alpha=0.7)\n",
    "ax.set_ylabel('Success Rate (%)')\n",
    "ax.set_title('Chaos Scriptorium: Standard vs LRS Agent')\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Volatility is real**: Production environments change unpredictably\n",
    "2. **Standard agents fail**: They retry the same action without adapting\n",
    "3. **LRS adapts**: Precision tracks confidence, triggers replanning\n",
    "4. **Automatic exploration**: Low precision ‚Üí try alternatives\n",
    "5. **Mathematical grounding**: No hardcoded thresholds or rules\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "The Chaos Scriptorium models:\n",
    "- **API rate limits**: Stripe API suddenly starts failing ‚Üí switch to cached data\n",
    "- **Service outages**: PostgreSQL down ‚Üí switch to MongoDB replica\n",
    "- **Permission changes**: S3 bucket becomes read-only ‚Üí switch to local cache\n",
    "- **Schema evolution**: API response format changes ‚Üí switch to alternative parser\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 5**: Integrate real LLMs for policy generation\n",
    "- **Tutorial 6**: Monitor agents with the dashboard\n",
    "- **Tutorial 7**: Deploy to production with Docker/K8s\n",
    "\n",
    "## Exercise\n",
    "\n",
    "1. Modify chaos parameters (interval, lock probability)\n",
    "2. Add a new tool with different failure characteristics\n",
    "3. Create your own volatile benchmark (e.g., flaky API simulation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
